{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
    "!pip3 install JPype1-py3 konlpy pandas tqdm\n",
    "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "!JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/fllff/nlp_basic_lec/master/ratings_train.txt',sep='\\t')\n",
    "\n",
    "# NaN 삭제\n",
    "df = df.dropna()\n",
    "\n",
    "# shuffle\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "mecab.morphs('한국어 토큰 분리기 사용방법')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    df.at[i,'document'] = mecab.morphs(row['document'])\n",
    "    \n",
    "print(df)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_list = df['document']\n",
    "label_list = df['label']\n",
    "\n",
    "doc_len_list = [len(doc) for doc in doc_list]\n",
    "\n",
    "# 문장길이 분포 확인\n",
    "distribution = [0]*20\n",
    "for doc_len in doc_len_list:\n",
    "    distribution[int(doc_len/10)] += 1\n",
    "print(distribution)\n",
    "\n",
    "# label균형 확인\n",
    "label_list.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 길이 trim\n",
    "doc_list = [doc[:50] for doc in doc_list]\n",
    "\n",
    "# 문장길이 분포 확인\n",
    "distribution = [0]*20\n",
    "for doc in doc_list:\n",
    "    distribution[int(len(doc)/10)] += 1\n",
    "print(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word_model = Word2Vec(doc_list, size=16, window=5, min_count=50, workers=2, iter=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.wv.most_similar('배우')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.wv['배우']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# token -> vector 변환\n",
    "vector_list = []\n",
    "for doc in tqdm(doc_list):\n",
    "    tmp = []\n",
    "    for token in doc:\n",
    "        try:\n",
    "            tmp.append(word_model.wv[token].tolist())\n",
    "        except:\n",
    "            tmp.append([0]*16)\n",
    "        \n",
    "    vector_list.append(tmp)\n",
    "    \n",
    "print(vector_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "tmp=[]\n",
    "\n",
    "for i, vector in tqdm(enumerate(vector_list)):\n",
    "    vector_list[i].extend([[0]*16 for x in range(MAX_LEN-len(vector))])\n",
    "\n",
    "print(len(vector_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "inputs = keras.Input(shape=(50,16))\n",
    "#x = layers.SimpleRNN(16)(inputs)\n",
    "x = layers.LSTM(16)(inputs)\n",
    "x = layers.Dense(1)(x)\n",
    "outputs = keras.activations.sigmoid(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n",
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "              metrics=['accuracy'],\n",
    "              loss=keras.losses.BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "model.fit(np.asarray(vector_list), \n",
    "          np.asarray(label_list), \n",
    "          batch_size=32, \n",
    "          epochs=3, \n",
    "          validation_split=0.01,\n",
    "          tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='batch'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '난 좀 지루한 느낌'\n",
    "\n",
    "test_vector = []\n",
    "for token in mecab.morphs(sentence):\n",
    "    try:\n",
    "        test_vector.append(word_model.wv[token].tolist())\n",
    "    except:\n",
    "        test_vector.append([0]*16)\n",
    "\n",
    "test_vector.extend([[0]*16 for x in range(MAX_LEN-len(test_vector))])\n",
    "print(test_vector)\n",
    "model.predict(np.asarray([test_vector]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
