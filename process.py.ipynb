{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
    "!pip3 install JPype1-py3 konlpy pandas tqdm\n",
    "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "!JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/fllff/nlp_basic_lec/master/ratings_train.txt',sep='\\t')\n",
    "\n",
    "# NaN 삭제\n",
    "df = df.dropna()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "mecab.morphs('한국어 토큰 분리기 사용방법')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    df.at[i,'document'] = mecab.morphs(row['document'])\n",
    "    \n",
    "print(df)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_list = df['document']\n",
    "label_list = df['label']\n",
    "\n",
    "doc_len_list = [len(doc) for doc in doc_list]\n",
    "\n",
    "# 문장길이 분포 확인\n",
    "distribution = [0]*20\n",
    "for doc_len in doc_len_list:\n",
    "    distribution[int(doc_len/10)] += 1\n",
    "print(distribution)\n",
    "\n",
    "# label균형 확인\n",
    "label_list.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 길이 trim\n",
    "doc_list = [doc[:50] for doc in doc_list]\n",
    "\n",
    "# 문장길이 분포 확인\n",
    "distribution = [0]*20\n",
    "for doc in doc_list:\n",
    "    distribution[int(len(doc)/10)] += 1\n",
    "print(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(doc_list, size=16, window=5, min_count=50, workers=2, iter=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('배우')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['배우']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# token -> vector 변환\n",
    "vector_list = []\n",
    "for doc in tqdm(doc_list):\n",
    "    tmp = []\n",
    "    for token in doc:\n",
    "        try:\n",
    "            tmp.append(model.wv[token].tolist())\n",
    "        except:\n",
    "            tmp.append([0]*10)\n",
    "        \n",
    "    vector_list.append(tmp)\n",
    "    \n",
    "print(vector_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "tmp=[]\n",
    "\n",
    "for i, vector in tqdm(enumerate(vector_list)):\n",
    "    vector_list[i].extend([[0]*16 for x in range(MAX_LEN-len(vector))])\n",
    "\n",
    "print(len(vector_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
